{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stop_words import get_stop_words\n",
    "from xgboost import XGBModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "russian_stop_words = get_stop_words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data reading and spliting on test and train\n",
    "data = pd.read_csv('data/text_BoW_task.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text_stem'],\n",
    "                                                    data['is_blocked'],\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train = X_train.astype(unicode);X_test = X_test.astype(unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the bag of word:\n",
    "vectorizer = CountVectorizer()#stop_words=russian_stop_words)\n",
    "train_data_features = vectorizer.fit_transform(X_train)\n",
    "test_data_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21605x54881 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 643008 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import xgboost as xgb\n",
    "##\n",
    "#  this script demonstrate how to fit generalized linear model in xgboost\n",
    "#  basically, we are using linear model, instead of tree for our boosters\n",
    "##\n",
    "dtrain = xgb.DMatrix(train_data_features)\n",
    "#dtest = xgb.DMatrix('../data/agaricus.txt.test')\n",
    "# change booster to gblinear, so that we are fitting a linear model\n",
    "# alpha is the L1 regularizer\n",
    "# lambda is the L2 regularizer\n",
    "# you can also set lambda_bias which is L2 regularizer on the bias term\n",
    "param = {'silent':1, 'objective':'binary:logistic', 'booster':'gblinear',\n",
    "         'alpha': 0.0001, 'lambda': 1 }\n",
    "\n",
    "# normally, you do not need to set eta (step_size)\n",
    "# XGBoost uses a parallel coordinate descent algorithm (shotgun),\n",
    "# there could be affection on convergence with parallelization on certain cases\n",
    "# setting eta to be smaller value, e.g 0.5 can make the optimization more stable\n",
    "# param['eta'] = 1\n",
    "\n",
    "##\n",
    "# the rest of settings are the same\n",
    "##\n",
    "watchlist  = [(dtrain,'train')]\n",
    "num_round = 4\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
    "preds = bst.predict(dtest)\n",
    "labels = dtest.get_label()\n",
    "print ('error=%f' % ( sum(1 for i in range(len(preds)) if int(preds[i]>0.5)!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LogisticRegression training on vectorised features\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(train_data_features, y_train)\n",
    "y_pred_1 = clf1.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935024065161\n"
     ]
    }
   ],
   "source": [
    "LogRegression_auc = accuracy_score(y_test, y_pred_1)\n",
    "\n",
    "LogRegression_accuracy = accuracy_score(y_test, y_pred_1)\n",
    "print LogRegression_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM training on vectorised features\n",
    "clf2 = LinearSVC()\n",
    "clf2.fit(train_data_features, y_train)\n",
    "y_pred_2 = clf2.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVM_auc = accuracy_score(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the bag of word with TF-IDF:\n",
    "vectorizer_Tfidf = TfidfVectorizer()\n",
    "train_data_features_tfidf = vectorizer_Tfidf.fit_transform(X_train)\n",
    "test_data_features_tfidf = vectorizer_Tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LogisticRegression training on vectorised features with TF-IDF transform\n",
    "clf1_tfidf = LogisticRegression()\n",
    "clf1_tfidf.fit(train_data_features_tfidf, y_train)\n",
    "y_pred_1_tfidf = clf1_tfidf.predict(test_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LogRegression_tfidf_auc = accuracy_score(y_test, y_pred_1_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM training on vectorised features with TF-IDF transform\n",
    "clf2_tfidf = LinearSVC()\n",
    "clf2_tfidf.fit(train_data_features_tfidf, y_train)\n",
    "y_pred_2_tfidf = clf2.predict(test_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVM_tfidf_auc = accuracy_score(y_test, y_pred_2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegression_auc:  0.935024065161\n",
      "SVM_auc:  0.92484265087\n",
      "LogRegression_tfidf_auc:  0.930025916327\n",
      "SVM_tfidf_auc:  0.91577193632\n"
     ]
    }
   ],
   "source": [
    "print 'LogRegression_auc: ', LogRegression_auc\n",
    "print 'SVM_auc: ', SVM_auc\n",
    "print 'LogRegression_tfidf_auc: ', LogRegression_tfidf_auc\n",
    "print 'SVM_tfidf_auc: ', SVM_tfidf_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
